<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Cyber Ape stories</title><link href="https://cyberape.space/en/" rel="alternate"></link><link href="https://cyberape.space/atom" rel="self"></link><id>https://cyberape.space/en/</id><updated>2025-01-05T00:00:00+00:00</updated><entry><title>Machines of smuggling faith</title><link href="https://cyberape.space/en/interpretations.html" rel="alternate"></link><published>2025-01-05T00:00:00+00:00</published><updated>2025-01-05T00:00:00+00:00</updated><author><name>FunBotan</name></author><id>tag:cyberape.space,2025-01-05:/en/interpretations.html</id><summary type="html">&lt;p&gt;How LLMs changed my view of religion and why they will never become AGI&lt;/p&gt;</summary><content type="html">&lt;p&gt;I’m pretty sure that everyone was taken by surprise when large language models (LLMs) blasted through the Turing test and emerged well on the other side with borderline superhuman abilities. A period of adjusting expectations and priorities then followed. For engineers like myself, it meant a competition to learn all the new tech the fastest and grab the juiciest contracts offered by the capitalists. And for the capitalists, it meant untold billions of investments into the AI industry. How could they pass this opportunity when the leaders of said industry, to whom these billions flow, promise to replace most of the current workforce with their products at a minuscule fraction of the cost?&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/interpretations/S-curves.png"&gt;&lt;/p&gt;
&lt;p&gt;Like every technology before, LLMs are going to follow the developmental S-curve (see the picture above) and are currently in the stage of exponential growth. Most of this growth thus far has been achieved by scaling up the same methods: in other words, more money directly translated to more capable models. Everyone knows this easy growth will end at some point, but the trillion-dollar question is: when?&lt;/p&gt;
&lt;p&gt;If the capitalists overestimate how high the curve goes and overcommit to the next step in scale, gigantic supercomputers worth their weight in gold and consuming more power than entire countries may turn into space heaters, and trillions of dollars invested into them and their entire supply chain may evaporate overnight. This would trigger an economic crisis that may forever transform how we see technology, and from which the tech industry may never fully recover. Yet far worse is the unlikely scenario if their bet succeeds and this leap of faith actually reaches the holy grail: artificial general intelligence (AGI), a machine that can do pretty much any work that humans can. God help us all if this happens under the current economic system.&lt;/p&gt;
&lt;p&gt;These dangers, however, are still theoretical, and there are more pressing problems to worry about right now. The one that feels most important to me is the effects LLMs are already having on education.&lt;/p&gt;
&lt;p&gt;Working with junior students, I observed a scary change in them since the release of ChatGPT. Of course, it is only natural that students will use any tools they can get their hands on to cheat the system: we’ve all been there and done that. However, it would be one thing if LLMs could just do any assignment better than the student: then the latter would, in the worst case, learn nothing. What I observe instead is that LLMs are damaging the capabilities that the students &lt;em&gt;used to have&lt;/em&gt;, up to and including the ability to think at all. This manifests as giving plausibly sounding non-answers to questions and a shallow approach to problem-solving that betrays a lack of an internal model of the subject. In other words, I observe the &lt;em&gt;students themselves starting to behave like LLMs&lt;/em&gt;. And I suspect that they are merely the canaries in the coal mine, the first mass adopters of the technology because the problems they face are perfect use cases for it.&lt;/p&gt;
&lt;p&gt;But why are they, exactly? Why is it that LLMs (and especially the new chain-of-thought models like o1-o3 from OpenAI) exhibit borderline superhuman abilities to solve problems &lt;em&gt;specifically designed to be solved&lt;/em&gt;, from textbook exercises to &lt;a href="https://x.com/__nmca__/status/1870170112290107540"&gt;the most complex math challenges ever made&lt;/a&gt;, yet fail miserably when confronted with real-life, open-ended problems without known solutions (as anyone who seriously attempted to replace human labor with LLMs will attest to)?&lt;/p&gt;
&lt;p&gt;One possible reason is that the amount of exercises, as well as detailed solutions for them, in the training data far exceeds that of real-world problems. This is because the academic and practical skills are learned in different ways: the former in a more standardized and documented way, the latter in a more ad-hoc, peer-to-peer, and &lt;em&gt;less linguistic&lt;/em&gt; fashion.&lt;/p&gt;
&lt;p&gt;Another explanation I am leaning towards is that, &lt;strong&gt;when designing problems-to-be-solved, people use some common heuristics that LLMs can pick up and exploit&lt;/strong&gt;. In geometric terms, the process of problem design traces a geodesic from the answer to the question on some latent manifold which humans are not consciously aware of, and all it takes a LLM to solve the problem is to retrace that line back. Of course, there are no such shortcuts to real-world problems.&lt;sup id="fnref:manifolds"&gt;&lt;a class="footnote-ref" href="#fn:manifolds"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The same conjecture is applicable to pretty much any benchmark, be that one designed for AIs or humans. That’s why I also suspect that it is impossible to design a reliable above-average intelligence test. And not coincidentally, the latest hype wave over LLMs was caused by a new record set by &lt;a href="https://arcprize.org/blog/oai-o3-pub-breakthrough"&gt;OpenAI’s o3 on the ARC-AGI benchmark&lt;/a&gt;, which is highly reminiscent of an IQ test.&lt;/p&gt;
&lt;blockquote class="reddit-embed-bq" style="height:316px" data-embed-height="316"&gt;&lt;a href="https://www.reddit.com/r/mensa/comments/1hprxxz/scored_135_first_time_taking_an_iq_test_not/"&gt;Scored 135 first time taking an IQ test - not because I’m a genius but because I’m an Electrical Engineer &lt;/a&gt;&lt;br&gt; by&lt;a href="https://www.reddit.com/user/Teflonwest301/"&gt;u/Teflonwest301&lt;/a&gt; in&lt;a href="https://www.reddit.com/r/mensa/"&gt;mensa&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async="" src="https://embed.reddit.com/widgets.js" charset="UTF-8"&gt;&lt;/script&gt;
&lt;p&gt;If this is indeed the case, then the incredible performance of LLMs is not at all what it seems, and it is a matter of time until our misinterpretation of their performance clashes with reality. An even worse way to put this is to liken LLMs to parasites precisely tuned to disrupt the very systems that provided their sustenance in the form of training data.&lt;/p&gt;
&lt;p&gt;But isn’t it about time for these systems to be disrupted?&lt;/p&gt;
&lt;h2&gt;The education system sucked anyway&lt;/h2&gt;
&lt;p&gt;You may be familiar with &lt;a href="https://en.wikipedia.org/wiki/Goodhart%27s_law"&gt;Goodhart's law&lt;/a&gt;, which states "When a measure becomes a target, it ceases to be a good measure". It captures a very general problem that we face when designing rules for self-regulating systems, be that exams for students, economic policies for nations or loss functions for machine learning models. The fundamental issue is that there is always a mismatch between what we want to achieve (the target) and what we incentivize (the measure), because if we could incentivize the target directly, we wouldn’t need the self-regulating aspect in the first place.&lt;/p&gt;
&lt;p&gt;The education system is a perfect example of this mismatch backfiring. I mentioned in the &lt;a href="embeddings.html#Platonic perspective"&gt;previous post&lt;/a&gt; that we don’t actually know how the education process works: we just teach people a bunch of random things, most of which aren’t directly useful, and hope that they will automagically develop useful abilities as a result. This approach did work for centuries, after all. But even before LLMs, the system was showing cracks.&lt;/p&gt;
&lt;p&gt;Firstly, standardized tests &lt;strike&gt;and their consequences have been a disaster for the human race&lt;/strike&gt; have long been a laughing stock for the already educated and a toxic swamp for the students who actually have to take them. The worst offender in my understanding (I never took this test myself and make conclusions from other peoples’ stories) is the Chinese Gaokao exam: not necessarily because it is a bad exam, but more due to how insanely competitive the wider Chinese environment makes it. It breaks my heart just imagining children who have to grind repetitive and largely pointless tasks for 15 hours a day instead of actually living a life. Even the standardized exam that I did take (the Russian YeGE), a much milder version of the same concept, had cases of students killing themselves after failing to pass. And for what? I am yet to meet a university professor who believes that the baseline education level of entrants improved after the introduction of the exam, and the majority firmly believe the opposite. Not to mention a billion-dollar exam cheating industry that sprawls in the shadows of every standardized test. This outcome is completely predictable from Goodhart's law: by making the measure (exam) standardized, we made it feasible to over-optimize for that measure at the expense of general educational attainment, which was the original target.&lt;/p&gt;
&lt;p&gt;Another sign of fundamental problems with the education system is the “gifted kids” phenomenon. In short, it turns out that if the education program (the measure) in the junior school years is too easy for a child, they will fail to develop the skills that are actually useful in life (the target) and end up at a disadvantage to their peers later when confronted with real problems.&lt;/p&gt;
&lt;p&gt;There is also a &lt;a href="https://sohl-dickstein.github.io/2022/11/06/strong-Goodhart.html"&gt;strong version of Goodhart's law&lt;/a&gt;, stating “When a measure becomes a target, if it is effectively optimized, then the thing it is designed to measure will grow worse.” The scary phenomenon of students behaving LLM-like that I described before is a clear manifestation of this.&lt;/p&gt;
&lt;p&gt;So, LLMs are not an out-of-the-blue gamechanger for the education system, but merely a straw that is about to finally break the camel’s back. They are the ultimate cheat code, one that can no longer be worked around by stricter proctoring and plagiarism detectors. They are the final call for the education system to change, lest it becomes obsolete.&lt;/p&gt;
&lt;p&gt;And the ideas how to reform it have been around for a long time. One is the western STEAM concept, another is &lt;a href="https://web.archive.org/web/20140419025050/http://spinoza.xclan.ru/evischool.pdf"&gt;a similar framework developed on the Soviet side of the curtain&lt;/a&gt;. Both emphasize abandoning memorization tasks in favor of developing independent thinking skills, teaching how to acquire relevant knowledge instead of knowledge itself, and putting hands-on experimental lessons before books. To me, all these recommendations were self-evident even during my own education, but implementing them would mean retraining every teacher, reequipping every school and rewriting all textbooks from scratch, all in an environment where the public education is barely scraping by as it is. Barring a worldwide communist revolution tomorrow, this is not likely to happen.&lt;/p&gt;
&lt;p&gt;Then the best chance we have to attenuate the disaster is to fight fire with fire. To counterbalance LLMs, we would have to develop some other AI systems that cannot be cheated as easily as humans. It sounds terrible, because it is: the list of potential ways this could go wrong is at least as long as this essay, and I don’t know how to avoid any of them, but it looks like we won’t have a choice but to figure it out.&lt;/p&gt;
&lt;p&gt;But before we can even attempt this, we have to return to the question I have been trying to answer for the last two years: &lt;em&gt;what will LLMs never be able to do?&lt;/em&gt; In the &lt;a href="embeddings.html"&gt;previous post&lt;/a&gt;, I derived the mathematical answer to it. In this one, I am going to approach from a completely different direction. Instead of focusing on hallucinations of LLMs, let’s instead notice that &lt;em&gt;humans&lt;/em&gt; also hallucinate and see how that presents an insurmountable barrier for LLMs.&lt;/p&gt;
&lt;h2&gt;I know that I know nothing&lt;/h2&gt;
&lt;p&gt;If you ask ChatGPT to solve some kind of problem without explaining the process, and then ask to explain the process in the next query, the explanation you get will have nothing to do with the solution before. In fact, they technically cannot be related, because LLMs have no internal state in which this information could have been saved.&lt;sup id="fnref:states"&gt;&lt;a class="footnote-ref" href="#fn:states"&gt;2&lt;/a&gt;&lt;/sup&gt; The explanation may coincidentally match the solution, but when it does not or it just doesn’t make any sense, we call it “hallucinated”. But the more I was thinking about this problem, the more I realized that it is not unique to LLMs: humans do the exact same thing all the time.&lt;/p&gt;
&lt;p&gt;To understand why, we will have to learn two words of Sanskrit today. These words are &lt;em&gt;jñāna&lt;/em&gt; and &lt;em&gt;vidyā&lt;/em&gt;, and both of them translate to “knowledge”. The difference is that vidyā means &lt;em&gt;transmissible&lt;/em&gt; knowledge: anything you can find in a book, film, podcast, forum or dataset. But there is, and that’s what LLM-optimists deny, a second kind of knowledge. Jñāna stands for the subjective, internal knowledge that cannot be exchanged with the outside world. Whenever you attempt to smuggle jñāna out, it simply turns into vidyā. And whenever a piece of vidyā helps you understand something, that is only because you already had the  jñāna to which it could attach. Not all vidyā is linguistic, but all language is vidyā. And the central claim of the Indic gnosiology is that new knowledge can &lt;em&gt;only&lt;/em&gt; be created as jñāna.&lt;/p&gt;
&lt;p&gt;This is superficially similar to Platonic gnosiology, but the difference is in the epistemology. While Plato, and the entire Western line of philosophy following him, emphasized linguistic reasoning as the means of uncovering objective truth, the Indic line instead focused on non-linguistic exploration of the subjective truths through practices. Could this be the reason that LLMs mesmerize us disproportionately to their real usefulness? Do we need to dig all the way down to antiquity to explain that? Probably not, but the thought is funny to entertain. And it is probably not a coincidence that &lt;a href="embeddings.html#Platonic perspective"&gt;Plato keeps popping up&lt;/a&gt; in this discussion.&lt;/p&gt;
&lt;p&gt;It’s not easy to bring up examples of jñāna, precisely because it is by definition unobservable and indescribable, but here is an imperfect I came up with. When you are trying to explain what you saw in a dream while still remembering it, your verbal explanation never quite matches what you actually saw. Like dreams, jñāna is produced in the subconscious, with only the end result popping up into consciousness. This is how we get “eureka moments”, or more generally, “shower thoughts”.&lt;/p&gt;
&lt;p&gt;Jñāna is the reason why we cannot actually teach people anything directly, we can only put them into situations where they are forced to generate their own jñāna. This ties directly into the problems of education I discussed in the previous section. Currently, the methodology and metrics of education systems are based on the assumption that the process of learning can be approximated as a mechanical accumulation of vidyā, but any teacher worth their salt knows that’s not even remotely true, that there is something much deeper going on. Ironically, the skill to teach people to acquire jñāna is in itself jñāna, but if we tried to at least think of it in the right way, perhaps we could come up with much better ways to organize the education process.&lt;/p&gt;
&lt;p&gt;Can LLMs have jñāna? I’m quite confident the answer is not. There are simply no mechanisms in their architecture from with something like that could emerge. All LLMs do is rearrange vidyā, and that is one more way to describe their fundamental limitation. Worse yet, all the vidyā they have is ultimately generated by humans as an attempt to “smuggle out” jñāna, and any results of this are fundamentally flawed, even if useful: much like every output of an LLM is a hallucination, just some of them happen to match reality. And if we just homogenize the terminology, we will be able to say that &lt;em&gt;humans hallucinate as well&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I realize this won’t be an easy idea to grasp, so let me describe a concrete example.&lt;/p&gt;
&lt;h2&gt;A surprise detour into religions&lt;/h2&gt;
&lt;p&gt;It’s not hard to guess my religious views, or lack thereof, from my writing. I might have been born a skeptic, as even being brought up in a moderately religious family, I had no inclination to faith whatsoever and merely cosplayed it until it was no longer necessary to conform to my parents’ expectations. But even as a purely ethical system, I had plenty of reasons to be offended by what Christianity had to offer. One thing I found unacceptable is the complete lack of moral value put on animals, whom I always viewed as not that different from us, lacking only the ability to speak (expect a separate post about it soon). On the other hand, I found classifying mental states like envy, pride, anxiety or depression as sins to be unfair: not only do they have no victims, but there is nothing I can do to stop them! Or so I thought.&lt;/p&gt;
&lt;p&gt;If you had read &lt;a href="neurodiversity.html"&gt;the short story&lt;/a&gt; I posted between these two big essays, you probably wondered what method allowed the protagonist to overcome their fear of water and turn their life around. I left it vague on purpose, but I will also admit that it is a metaphor for my own experience, and for me that method was Yoga. After taking it moderately seriously for just a year, I already learned to stop panic attacks and the spiral of depression, to cognitively reframe every misfortune on the fly and enter the flow state on demand, things that I would have previously seen as superpowers; and this is only the tip of the iceberg. Within this system, it makes total sense to classify envy, pride, anxiety or depression as faults of an individual, &lt;em&gt;because it gives that individual the power to overcome them&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You see, the thing that positively distinguishes the Indic spiritual tradition (Yoga, Buddhism, Hinduism, etc.) from the Abrahamic one, and why faithless skeptics like myself flock to it, is that it offers &lt;em&gt;practices&lt;/em&gt; (what we collectively refer to as “meditation”) that can be followed to achieve objective results without believing in anything supernatural. Problem is, no one can explain in language how these practices work.&lt;sup id="fnref:sanskrit"&gt;&lt;a class="footnote-ref" href="#fn:sanskrit"&gt;3&lt;/a&gt;&lt;/sup&gt; They were developed by sheer brute force without any theoretical framework to predict what will and what won't work analytically.&lt;/p&gt;
&lt;p&gt;And now I am going to make a leap of logic with a high risk of being off. Unfortunately I can’t afford to get a degree in religion studies just to finish this post, so anyone with a deeper knowledge of the subject is welcome to tell me if this is either a well-known fact or does not fit the data. Still, here goes.&lt;/p&gt;
&lt;p&gt;I suspect that the spiritual practices are what came first, most likely developing independently many thousands of times, and preceding the development of a class society (in the historical materialist sense). Initially they were passed down in a peer-to-peer, language-invariant manner. However, as the societies developed and the demand for this knowledge grew, teachers had to start putting it into &lt;em&gt;language&lt;/em&gt;, but the best they could produce were post-hoc explanations without much useful insight into the underlying processes. These explanations were then passed through oral tradition by people who did not actually participate in the practices, and what eventually came out of this game of telephone was religious mysticism and religions themselves.&lt;/p&gt;
&lt;p&gt;As long as the practices were passed down uninterrupted, the practitioners could adjust the mysticism to at least not contradict reality. However, &lt;em&gt;something&lt;/em&gt; happened with the development of class society. At some point, these primordial practices that worked without any preconditions were &lt;em&gt;purged&lt;/em&gt;, at least from the Abrahamic religions. The likely victims of this purge were the Jewish Kabbalists, Christian Gnostics and Muslim Sufists: largely because the absence of information about them can only be explained by the deliberate and coordinated destruction of it. The likely motivation for the purge on the part of the ruling class was to increase their power by monopolizing spirituality. If we analyze, say, Christianity through the lens of this hypothesis, many things start to make sense, including those pesky mental sins.&lt;/p&gt;
&lt;p&gt;I can’t say if the same process happened in other regions and religious traditions, only that in happened to the Abrahamic one and did not happen to the Indic one. The latter could be a result of the Indian caste system being somewhat different from the more common class structure that developed in Europe and Middle East.&lt;/p&gt;
&lt;h2&gt;Hold my dataset&lt;/h2&gt;
&lt;p&gt;So, why did I even go on this tangent about religions?.. Ah yes, LLMs. My point in short is that &lt;em&gt;religious mysticism is a hallucination&lt;/em&gt;, if we use the same definition of hallucinations consistently for humans and LLMs. Why is this important for the discussion? We’ve now established that humans hallucinate too, and this presents a big problem: it means that &lt;em&gt;not all data generated by humans, regardless of quality, can be trusted as examples of reasoning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Religious mysticism is merely the clearest example. If I had to generalize, I would say that the class of potentially untrustworthy data are &lt;em&gt;interpretations&lt;/em&gt;, meaning post-hoc explanations for practices. Another example that immediately jumps to mind are the interpretations of quantum mechanics. The reason they are useless as reasoning data is because you cannot draw any new conclusions from the interpretations alone (those who do end up neck-deep in quantum mysticism), you need to do the actual math for that.&lt;/p&gt;
&lt;p&gt;So, we know that LLMs hallucinate their reasoning for generating a specific response. But we also know that humans do the same. What follows is that &lt;em&gt;LLMs are sometimes learning human reasoning from explanations of it that humans themselves hallucinated!&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The one question left is: how much do humans hallucinate and what percentage of training data out there can be classified as hallucinations? It is easiest to answer it from contrariwise: to identify which data is definitely not hallucinated. The only texts that meet this criterion are the ones that are not interpretations, i.e. not written to explain something after the fact. But if you try to think about it, almost all text ever written is written after the thing it is describing had already happened. The only exceptions are the text which are self-contained and do not refer to the material reality at all: mathematical problems with their solutions, some source code and perhaps the most abstract strains of philosophy. An even smaller subset of that small subset, problems with objectively verifiable solutions, are what the o1-o3 model series are trained on. It is also not coincidental that all previous instances of AI systems outperforming humans happened in self-contained problems with known rules, such as in games.&lt;/p&gt;
&lt;p&gt;So, I will make a grand statement: &lt;strong&gt;LLMs cannot reason about the material world and will never be able to do so&lt;/strong&gt; due to the nature of how humans produce language. This is to say nothing about future AI systems that gain an ability to interact with the material world directly, without text mediation. They will have their own problems, but we’ll talk about those sometime later.&lt;/p&gt;
&lt;p&gt;Combining this with the conclusion of the first section, we arrive at a picture that is very similar to the present-day state of the art: LLMs excel at toy problems, exercises and benchmarks, but struggle to produce anything of real material value.&lt;/p&gt;
&lt;p&gt;It’s hardly surprising that people are seeing LLMs cracking problems that only a handful of humans could solve before and assume that their performance on real-life tasks will be comparable. But this is a mistake that comes from anthropomorphizing algorithms, a mistake that we have made many times before.&lt;/p&gt;
&lt;p&gt;Remember when playing chess was used as a measure of intelligence? No? Me neither. I was born around the time that Deep Blue defeated Garry Kasparov, proving that winning chess does not actually require intelligence, because Deep Blue clearly did not posses it. And &lt;a href="https://youtu.be/30D00BDvfTA?si=iKETtHUZ8CY9H9jy"&gt;far worse metrics&lt;/a&gt; have been used before. Something tells me that, in a decade or two, we will be viewing contest math problems as something not very different from chess: as powerlifting for the brain; something you do purely for self-improvement. After all, we don’t hire powerlifting champions to actually lift stuff, we just use cranes.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:manifolds"&gt;
&lt;p&gt;Yes, deep neural networks can solve different real-world problems too by uncovering and exploiting latent manifolds. What I am saying here is that the same method applied to problems-to-be-solved uncovers manifolds of the &lt;em&gt;minds of test designers&lt;/em&gt; rather than the subjects of those tests. This works as a solution to test problems, but cannot generalize to real-world ones.&amp;#160;&lt;a class="footnote-backref" href="#fnref:manifolds" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:states"&gt;
&lt;p&gt;Of course, I can envision a CoT model using its internal chain of thought saved from the first query to answer the second. In this setup, we would have to ask a different question: can the model explain how it got from one step to another within its chain of thought, which will also produce a hallucinated explanation.&amp;#160;&lt;a class="footnote-backref" href="#fnref:states" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:sanskrit"&gt;
&lt;p&gt;Sanskrit claims to be able to do that, but I do not have the knowledge to judge how effective it is. And even if it is effective, it is largely untranslatable to natural languages, so even if a Sanskrit-speaking LLM is trained, it would not be able to leverage the power of the shared translinguistical latent space to manipulate it.&amp;#160;&lt;a class="footnote-backref" href="#fnref:sanskrit" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="interpretations"></category></entry><entry><title>I Have No Flippers and I Must Swim</title><link href="https://cyberape.space/en/neurodiversity.html" rel="alternate"></link><published>2024-09-01T00:00:00+01:00</published><updated>2024-09-01T00:00:00+01:00</updated><author><name>FunBotan</name></author><id>tag:cyberape.space,2024-09-01:/en/neurodiversity.html</id><summary type="html">&lt;p&gt;This is a story about (one aspect of) neurodiversity&lt;/p&gt;</summary><content type="html">&lt;p&gt;Imagine a tropical archipelago of a view dozen islands in the middle of an enormous ocean, far away from any other land. The islands are close to each other, and the water between them is shallow and clean enough that you can always see the vibrant reef ecosystem at the bottom.&lt;/p&gt;
&lt;p&gt;These islands are inhabited by a stone age tribe of a few thousand people, but not quite like us: they have mild biological adaptations for swimming, like increased blood reserves to hold breath for longer, a third eyelid for seeing underwater, and membranes between fingers. Most importantly, they are born with the ability to swim: their toddlers can normally swim before they can walk. That is, &lt;em&gt;all except you&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Somehow — no one in your lifetime will understand why — you were born without that ability. Nothing else was wrong with you. In principle, you could easily learn to swim. But in practice, there were two insurmountable issues.&lt;/p&gt;
&lt;p&gt;First, no one had ever taught or learned swimming before. So, as much as the others may have wanted to help, they could not decompose the swimming skill into teachable steps, because for them there were no steps: it was a single action that you “just do”. It would be like teaching someone how to breathe or to sleep. “It can’t be that hard.”&lt;/p&gt;
&lt;p&gt;You could conceivably teach yourself through trial and error if not for the second issue. Since it was assumed that all children would be able to swim once released into the water, your family learned about your disability the hard way. Before they finally realized that you couldn’t do it, you already developed a deep-rooted fear of water through repeated drownings. Thus, the crucial step in learning to swim, which is to relax in the water and float on it, was impossible for you.&lt;/p&gt;
&lt;p&gt;For the culture in which you were born, swimming meant everything. An average tribe member would spend more time in a day in water than on land. They subsisted primarily by spearfishing in the reefs, swam from one island to another whenever they needed to communicate with the rest of the tribe, and generally led most of their social lives in the sea.&lt;/p&gt;
&lt;p&gt;But it gets worse. Because your inability to swim was entirely in your head, many accused you of lying about your condition to gain… Something? In a way, it would be easier if your disability was physical, like paralysis: at least, that way, you would be believed. But with things as they were, you inevitably became an outcast.&lt;/p&gt;
&lt;p&gt;This, combined with watching other kids gracefully gliding through the water, playing, and learning to make their living, filled you with a heavy baggage of emotions on top of what you already had to deal with. Resentment. Envy. Self-hatred. Suicidality.&lt;/p&gt;
&lt;p&gt;But at least you still had a loving family, even if you could not feel love for anyone in that state of mind. They spent years searching for anything that would help. And while most of their “solutions” came down to useless religious obscurantism, one of them actually worked. They suggested that you become a carpenter.&lt;/p&gt;
&lt;p&gt;In your culture, woodwork was the kind of work that no one really wanted to do if they had options. With no metal tools or good wood available, it was tedious and menial labor that you also had to do on land, under the scorching tropical sun, and away from the comforting embrace of the sea. Additionally, learning it inevitably meant painful cuts and splinters, especially in the finger membranes. But for you, it was the best opportunity to earn a living: craft the things others need and trade them for food. So, a carpenter you became, and eventually, a damn good one at that.&lt;/p&gt;
&lt;p&gt;Years passed as things settled down into this imperfect equilibrium. The job you thought you’d hate instead turned out to be quite fun and engaging, so eventually, you ended up building your own projects in your spare time.&lt;/p&gt;
&lt;p&gt;One type of request you often got was to build a raft. In your culture, a raft was just a few logs tied together to form a flat platform. It was moved by people in the water pushing or pulling on it and was only used to carry light items between islands, such as tools or food. They were satisfied with it, but you wondered if you could improve the design. And so you tried, at first, with scale models that you could test in the small, non-frightening freshwater creek. A big leap in performance happened when you switched from assembling a raft from multiple twigs to hollowing out one log. After a few dozen iterations, you devised a boat that was far more stable in the waves and could carry much more weight than a raft. And eventually, you made the full-scale version, albeit without a clear idea of what to do with it since your fear of water was too strong to test it out yourself.&lt;/p&gt;
&lt;p&gt;Not everything went well, of course. Some members of the tribe considered the boat to be an insult to the water gods. They threatened you and eventually burned it. And this might have been the end of the story if fate didn’t strike soon thereafter.&lt;/p&gt;
&lt;p&gt;The only freshwater spring on the island dried out. This problem could render the island uninhabitable if it was not solved quickly. Of course, those who destroyed your first boat immediately made the connection that it must have upset the water gods, and they bestowed this catastrophe as punishment. But there was another, much more logical connection. The other islands would gladly share their water, but there was no way of transporting it at the necessary scale — except for a boat.&lt;/p&gt;
&lt;p&gt;The voting options at the island assembly were “Let you build another boat” and “Toss you into the ocean to appease the gods”. Luckily for you, cooler heads prevailed. You and the few other carpenters worked day and night to build the vessel, accidentally inventing paddles in the process. And while you could not sail yourself, no one would deny that the promptly arriving water shipment was your achievement. For the first time ever, you felt like you belonged.&lt;/p&gt;
&lt;p&gt;Eventually, people would dig a well to provide a more reliable supply of water for the island, but boat communication continued for trading other heavy goods that were uneconomic to ferry before.&lt;/p&gt;
&lt;p&gt;One day, a guest from another island stepped off a boat and said he was looking for you. And the news he brought was about to change your life.&lt;/p&gt;
&lt;p&gt;First, you were not alone. He was born with the same condition as you. And, in all likelihood, there were others. Swim-capable people just preferred not to bring up this topic when they visited other islands: maybe they felt shame, maybe they feared they did something wrong to cause your condition. And those like you had no opportunity to meet, until now.&lt;/p&gt;
&lt;p&gt;“But how,” — you asked — “could you get on the boat? Aren’t you afraid of water as well?” “I was,” — the guest replied — “but I found a way to transcend the fear. It’s not quick or easy, but I will teach you if you want.” How could you say no to that?&lt;/p&gt;
&lt;p&gt;And from there, things mostly went uphill. In a year, you could board your own boat for the first time, and in five, you finally taught yourself to swim. Ironically, it was not nearly as important to you by then because you had already built a fleet of boats and a community of people who accepted you the way you were, with or without that ability.&lt;/p&gt;
&lt;p&gt;As years turn into decades and decades turn into centuries, your descendants invent the sail, celestial navigation, and other technologies that allow them to venture into the open ocean. There, the capacity to swim that once defined them becomes vestigial, only briefly useful when someone accidentally falls overboard. While non-swimmers are no longer disadvantaged, there is always some other group of people who are.&lt;/p&gt;
&lt;p&gt;A fish cannot conceptualize water until it is beached. Likewise, a socially adjusted person cannot understand the limitations of the society to which they are adjusted. It takes a different perspective to imagine a different reality, and then it takes a lot of one-sided fighting to make it happen. But when it does happen, everyone ends up benefiting.&lt;/p&gt;</content><category term="neurodiversity"></category></entry><entry><title>Embeddings, Aufhebung &amp; Denkökonomie</title><link href="https://cyberape.space/en/embeddings.html" rel="alternate"></link><published>2023-12-12T00:00:00+00:00</published><updated>2023-12-12T00:00:00+00:00</updated><author><name>FunBotan</name></author><id>tag:cyberape.space,2023-12-12:/en/embeddings.html</id><summary type="html">&lt;p&gt;Connecting some dots between machine learning, neurobiology, and the 19th-century German philosophy&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;The problem with the contemporary discourse around AI is that there's a near-zero intersection between those who have been trying to analytically understand intelligence for millennia and those who went the "reverse-engineering" route. And the results are absolutely comical on both sides.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Machine learning perspective&lt;/h2&gt;
&lt;p&gt;Without knowing anything about the nature of colors, we may naively assume that each hue (red, green, blue, yellow, purple, etc.) exists independently of the others. However, having started mixing colors, we quickly (or not so quickly) come to the conclusion that there are only three truly independent colors: red, green, and blue, and all the rest are their &lt;em&gt;linear combinations&lt;/em&gt;&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; (mixtures in different proportions).&lt;/p&gt;
&lt;p&gt;Even for those with little knowledge of mathematics, it should not be difficult to imagine colors as three-dimensional vectors (lists of three numbers). But what is the benefit of this representation? Firstly, we obtain a correspondence between the physical process of color mixing and arithmetic operations on vectors. Secondly, this representation is also maximally &lt;em&gt;compressed&lt;/em&gt;; it takes up the minimum possible amount of memory.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/rgb.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The example of colors is good precisely because we know the answer in advance. But there are many other classes of objects for which we would like to obtain a representation with similar properties, but how to do this is not at all obvious. One such class could be &lt;em&gt;words&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Again, we can start with the assumption that all words are independent of each other; that is, in the linear space of words, each of them corresponds to its own dimension (this approach is called one-hot encoding). But this assumption will quickly be shattered by fairly obvious examples of antonyms, such as “high - low,” “bright - dim,” “help - hinder,” “loud - quiet.” For each pair of antonyms, we can write down an equation of the form “high + low = 0”, “help + hinder = 0”, etc. In this way, we algebraically &lt;em&gt;express&lt;/em&gt; one word through another, which means we can eliminate one of the two dimensions originally allocated to them. Moreover, the right side of these equations needs not be zero. It could also be a non-zero vector: &lt;em&gt;another word&lt;/em&gt;. For example: “damp + cold = dank”, “irony + mockery = sarcasm”, “music + poetry = song”. Moving up a level to four words in one equation, we can begin to illustrate the different kinds of relationships between words. For example, “king - man + woman = queen” is a semantic relationship, and “big + less = small + more” is a syntactic one. These are just the simplest examples; there is no limit to the complexity of such constructs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/words.png"&gt;&lt;/p&gt;
&lt;p&gt;The problem is that considering all possible verbal equations for expressing words through each other and determining the relative positions of their corresponding vectors is a task that is far beyond reasonable in terms of labor intensity. Which is why it was solved only with the rise of machine learning. A breakthrough in word vectorization was the simply named &lt;a href="https://arxiv.org/abs/1301.3781"&gt;word2vec algorithm&lt;/a&gt;, published a decade ago. The general public might remember it from some meme-worthy examples of verbal equations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pig - oink + Santa = HO HO HO&lt;br&gt;
pig - oink + Woody Woodpecker = Yabba dabba doo&lt;br&gt;
pig - oink + Fred Flinstone = wassup&lt;br&gt;
pig - oink + Homer Simpson = D’oh&lt;br&gt;
pig - oink + Donald Trump = YOU’RE FIRED&lt;br&gt;
pig - oink + Einstein = E = mc2&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Along with machine learning, a new word has appeared to describe such vector representations of objects: &lt;strong&gt;embeddings&lt;/strong&gt;, also known as latent vectors, since the linear space they occupy is called &lt;strong&gt;latent space&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The algorithm for generating embeddings turned out to be rather indirect. It consists of training a model, usually a neural network, to &lt;em&gt;guess&lt;/em&gt; information. For example, in the case of words, we can remove one word from a sentence and have the model &lt;em&gt;predict&lt;/em&gt; what should be in its place. Embeddings are generated during the training process as the model’s internal representations of the data that it is processing. If this explanation was confusing, then you only have to remember one thing: the connection between embeddings and predictions. We will need this connection soon.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/translation.png"&gt;&lt;/p&gt;
&lt;p&gt;An interesting property of language is that embeddings of words from different languages will form a very similar structure, and by overlaying the latent spaces of different languages, one can create a dictionary for translation between them &lt;em&gt;without a single example of actual translation&lt;/em&gt;. This property has been known for a long time, albeit in a different formulation, and was used by archaeologists to decipher dead languages long before machine learning. It indicates that embeddings are not arbitrary but are an objective property (or rather, a homomorphic image) of what they are encoding. So, in the limit, different methods for calculating embeddings of the same objects should theoretically converge to similar results.&lt;/p&gt;
&lt;p&gt;With the help of machine learning, we can calculate embeddings for anything, provided that we have a sufficient number of examples illustrating relationships between the objects in question. For example, texts can be used to construct word embeddings, but you can also embed higher-level structures: sentences, paragraphs, and entire articles. They will simply require many more examples. Images are a little more complicated, but if you ever solved a captcha like "select all squares that contain X to prove you're not a robot", you helped some corporation (most likely Google) create better image embeddings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/captcha.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Embedding technology is also a pillar of all breakthroughs in the field of artificial intelligence in recent years, which is not surprising: embeddings, in essence, are a portal between the real world of people and the digital world of machines. ChatGPT and other generative neural networks like Stable diffusion and Midjourney rely on them.&lt;/p&gt;
&lt;p&gt;And speaking of image-generating models, have you ever seen &lt;a href="https://www.youtube.com/results?search_query=latent+walk"&gt;videos&lt;/a&gt; that consist of images endlessly morphing into each other, often in disturbing ways? It is &lt;a href="https://keras.io/examples/generative/random_walks_with_stable_diffusion/"&gt;also possible&lt;/a&gt; to specify a starting and a final image, and a model will interpolate an uncannily smooth transition between them, no matter how different they are. The fact that this is possible means that there is a nearly infinite number of images in between any two. This notion of &lt;em&gt;something being between two images&lt;/em&gt; is the key to understanding the nature and limitations of creativity that contemporary generative AI exhibits.&lt;/p&gt;
&lt;p&gt;Because, large as it may be, the latent space mathematically cannot contain all possible images or texts: their numbers are many orders of magnitude higher, and the vast majority are just noise anyway. So, what does it contain? It includes all possible things you can get by recombining the pre-existing data in various ways. All possible results of such recombinations taken together are called the &lt;strong&gt;span&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So then, what makes the creativity of GenAI different from that of humans? GenAI is locked within the span of the data it was trained on. This span is mindbogglingly large and has enough samples that will appear original to us. On the other hand, not all human artwork is original either: most of it is also a remix of what came before. But a human can, at least in principle, create something outside this span. If this weren't the case, it would be impossible for human creativity ever to begin because there would be no initial span for the very first artists or writers to sample from.&lt;/p&gt;
&lt;p&gt;Of course, modern GenAI does much more than walk across the latent space: LLMs answer your questions, and image generation models create pictures from a textual description. But these abilities really just come down to doing fancier math in the latent space to find what the user is looking for more efficiently. The fundamental limit on GenAI creativity remains the same and will remain the same until we come up with a completely new technological paradigm for AI.&lt;/p&gt;
&lt;p&gt;The problem is that there is not even a universally accepted explanation for why the paradigm we currently have is working to the extent it does. However, one explanation that I personally subscribe to is the &lt;a href="https://en.wikipedia.org/wiki/Manifold_hypothesis"&gt;Manifold hypothesis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The simplest example of a (low-dimensional) manifold is a sphere. While the sphere itself is a 3D object, its surface is 2D, meaning that we can uniquely identify any point on it with only two numbers. And even if we are only given 3-dimensional vectors describing points on this sphere, given enough of them, we should be able to reconstruct the complete sphere and remap the points to the two-dimensional surface, again &lt;em&gt;compressing&lt;/em&gt; information.&lt;/p&gt;
&lt;p&gt;What the manifold hypothesis posits is that deep learning is doing exactly that. First, it assumes that all &lt;em&gt;naturally occurring and useful&lt;/em&gt; data types intrinsically exist on low-dimensional manifolds inside the spaces where we find them: for example, meaningful sentences are a manifold within the space of all possible strings, and images we can recognize as pictures are a manifold within all possible combinations of pixels. The task of the neural network is then to find these manifolds and flatten them into linear spaces, which we already know as &lt;em&gt;latent&lt;/em&gt; spaces. In fact, the extracted and flattened manifold is nothing else than the already familiar &lt;em&gt;span&lt;/em&gt;, and the data points remapped to it are nothing else than &lt;em&gt;embeddings&lt;/em&gt;.&lt;/p&gt;
&lt;h3&gt;Applications and implications&lt;/h3&gt;
&lt;p&gt;By formal definition, embeddings are just vectors whose geometric distance is proportional to the similarity of the objects they represent (an isometric homomorphism, if you will). Essentially, computing embeddings is the task of &lt;em&gt;classifying&lt;/em&gt; objects and determining their degree of relatedness, whatever that means in each specific case.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/sentences.png"&gt;&lt;/p&gt;
&lt;p&gt;This property of correspondence between geometrical distance in latent space and similarity of embedded objects can be leveraged to perform similarity search. First, you build a database of embeddings (called a vector database) as a key-value storage, with vectors being keys and original objects being values. Then, you can run any new object through the same model to obtain its embedding. Finally, you do a K-nearest-neighbors search on that embedding in the database and return the values corresponding to found keys. This is roughly how image search, facial recognition, and recommendation algorithms work. Even most text search engines already use embeddings to search not only for literal matches with the search query but also for texts that are similar in meaning but phrased differently. Such search is called &lt;em&gt;semantic&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/vdb.png"&gt;&lt;/p&gt;
&lt;p&gt;But, as I briefly mentioned at the beginning, a side effect of computing embeddings is the compression of information&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. And there is some reason to believe that this is not just a side effect but an &lt;em&gt;equivalent definition&lt;/em&gt; of embeddings. This reason is an incredibly interesting &lt;a href="https://aclanthology.org/2023.findings-acl.426/"&gt;article&lt;/a&gt;, which proposes a way to measure distance between texts using compression (based on the notion of &lt;a href="https://brilliant.org/wiki/kolmogorov-complexity/"&gt;Kolmogorov complexity&lt;/a&gt;), thereby allowing the use of compressed texts as quasi-embeddings. It then compares embeddings produced by the most powerful language models with a conventional Gzip archiver in the text classification task. Paradoxically, in many tests (especially on small samples and outside the training distribution), Gzip's performance came close to much more sophisticated language models.&lt;/p&gt;
&lt;p&gt;If that made no sense to you, then here is the conclusion in simpler words. Classifying objects requires identifying their common features, but if we have found such features, then we no longer need to remember them for each object separately; it is enough to have one record for all objects of the same class. Conversely, the task of information compression requires a reduction of repeating sequences, which turn out to be common properties of objects. In other words, both tasks come down to &lt;em&gt;generalization&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But notice that neither machine learning nor even embeddings themselves appear in the previous paragraph. This is because it uncovers a much more general principle that applies, among other things, to the human brain. Evidence of this can be found in research from the field of mind reading: this year alone, two sci-fi-level results were obtained there.&lt;/p&gt;
&lt;p&gt;The first is the &lt;a href="https://www.nature.com/articles/s41593-023-01304-9"&gt;semantic reconstruction of language&lt;/a&gt;: decoding heard, read, and even imaginary (internal) speech from brain scans. Remarkably, the developed system does not reproduce words exactly but in a slightly different formulation that preserves the general meaning, hence the “semantic” qualifier. This means that it reconstructs &lt;em&gt;the thoughts&lt;/em&gt; themselves and not, for example, the motor signals that the brain involuntarily sends to the tongue and larynx, even when it is not speaking out loud. But most interestingly, if the same system is used on a person who is watching a silent film, the result of decoding their thoughts will be a text description of the scenes of this film!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ai.meta.com/blog/brain-ai-image-decoding-meg-magnetoencephalography/"&gt;The second result&lt;/a&gt; is a similar system but for the reconstruction of images, visible or imaginary. Its properties are the same: the images do not match exactly, but they are similar in a rather curious way, reflecting what details the experimental subject is paying attention to.&lt;/p&gt;
&lt;p&gt;Both systems share a very similar architecture and contain two key elements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generative neural network. Semantic language reconstruction uses GPT-1, and image reconstruction uses a diffusion model from the same class as Stable diffusion and Midjourney.&lt;/li&gt;
&lt;li&gt;A model of the subject’s brain, implemented as an artificial neural network that, based on data (text or picture), predicts what signals the brain that thinks about this data will produce. I was amazed by the fact that this is already possible: in theory, modeling the human brain should not be achievable with the current level of technology. However, the models exist, and they work. Even if this is just a rough approximation at the moment, the very fact that such an approximation is possible and useful makes a big difference in estimating the complexity of this problem. Fortunately, to obtain results, the model must be trained individually on each subject, which is impossible without their cooperation, so it’s yet too early to worry about the adoption of mind reading by states and corporations for nefarious purposes. &lt;em&gt;Yet.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The system works as follows. First, the generative neural network creates many different variations of text or images. These options are fed to the neural network that models the brain, and it generates corresponding signals. They are compared with real brain scans of the subject, and the most similar ones are selected. The data that generated these signals arrives at the output of the system as the decoded result, but at the same time is transmitted back to the generative model, which generates suitable continuations to repeat the process.&lt;/p&gt;
&lt;p&gt;All this is strikingly reminiscent of the semantic search mechanism described above. If we treat brain states as analogs of embeddings, then the process of mind decoding is just a similarity search in the space of these embeddings. Indeed, earlier studies independently confirmed that &lt;a href="https://www.nature.com/articles/nature21692"&gt;the brain also uses the concept of latent space&lt;/a&gt;, and &lt;a href="https://proceedings.neurips.cc/paper/2018/file/99064ba6631e279d4a74622df99657d6-Paper.pdf"&gt;the process of memory consolidation in the hippocampus is strikingly similar to the computation of embeddings&lt;/a&gt;. Particularly amazing is a &lt;a href="https://arxiv.org/pdf/2112.04035.pdf"&gt;paper showing equivalence&lt;/a&gt; between the hippocampus model and transformers, a class of artificial neural networks that underlie recent breakthroughs in natural language processing (that’s what T in GPT stands for) and which were developed without any prior knowledge of neurobiology. So, science once again converged to a solution that nature had already invented&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;. But this raises another question: what problem was nature solving?&lt;/p&gt;
&lt;h2&gt;Neurobiological perspective&lt;/h2&gt;
&lt;p&gt;On the one hand, the adaptive value of the human brain cannot be overestimated (says a human brain, ha-ha), which means that its development should have been very strongly encouraged by natural selection. This is supported by its relatively rapid evolution. On the other hand, biophysics imposes strict constraints on the parameters of the brain. These are, firstly, physical dimensions, limited by the constraint of having to be born. Secondly, the human brain already consumes a fifth of the body’s total energy, and energy in nature is a strictly limited resource, so there was no opportunity to freely increase its consumption until quite recently, by evolutionary standards, with the emergence of agriculture. Thus, we have a very acute contradiction between the need to increase the power of the brain and the limitations on the parameters by which this could be achieved. It then follows that &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5651807/"&gt;the evolution of the brain should have gone in the direction of increasing &lt;em&gt;efficiency&lt;/em&gt;&lt;/a&gt;. And indeed, it seems that &lt;a href="https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know"&gt;it is already about as efficient as it can theoretically be&lt;/a&gt;. Many other human adaptations &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9197885/"&gt;that increase energy efficiency at the expense of brute force&lt;/a&gt; hint at the same direction.&lt;/p&gt;
&lt;p&gt;But how is this efficiency achieved? For the answer, we again have to turn to machine learning, which &lt;a href="https://arxiv.org/abs/1605.08104"&gt;found&lt;/a&gt; that if you constrain the model in the energy budget, it automatically produces a predictive coding mechanism: the same one that produces embeddings! Moreover, transferring this result from artificial neural networks to biological ones is not a leap of faith: we already know for sure that predictive coding mechanisms arise in almost any well-optimized part of the brain.&lt;/p&gt;
&lt;p&gt;Since humans spend most of their time communicating with others (or at least did so until they invented the damned Internet), optimizing social interaction has been a high priority for evolution. The result of this optimization was, on the one hand, a &lt;a href="https://pubmed.ncbi.nlm.nih.gov/12921766/"&gt;highly specialized language cortex&lt;/a&gt;, which &lt;a href="https://www.nature.com/articles/s41562-022-01516-2"&gt;constantly tries to predict what it will hear&lt;/a&gt;, and on the other, mirror neurons, which completely model the mental state of other people (with proper development, not only people), and therefore they inevitably operate with compressed representations, since no system can completely model itself.&lt;/p&gt;
&lt;p&gt;If you reflect on the subtleties of your own behavior, you may notice these systems at work every time you interact with information in any way. For example, when opening an article, you may semi-subconsciously glance at the author, and if you already know them, then your brain will quickly make a prediction of what you are going to read about using the information it remembered from previous articles by the same author. This decreases the energy required to process the text and makes the reading process less frustrating. But this may also lead you into an echo chamber if you don’t make a conscious effort to expend more energy, much like with exercise.&lt;/p&gt;
&lt;p&gt;Incidentally, the part of your brain that makes these predictions works in a way not unlike a generative neural network such as GPT, at least on the surface level. We &lt;a href="https://memory.ucsf.edu/symptoms/speech-language"&gt;know for a fact&lt;/a&gt; that generating speech or text is handled by a different brain structure than analyzing it: Broca’s area and Wernicke’s area, respectively. This is what Hemingway’s famous advice "write drunk, edit sober" captures. This also hints at another limitation of contemporary generative language models, as well as at how to overcome it. Instead of trying to build one model that can, so to speak, write and edit at the same time, we should mimic the brain structure and develop a different type of model that works similarly to Wernicke’s area and then connect them.&lt;/p&gt;
&lt;p&gt;“What I cannot create, I do not understand,” said Richard Feynman. Conversely, the best way to understand something beyond any doubt is to create it. So then, isn’t the creation of artificial intelligence our most successful attempt to understand our own?&lt;/p&gt;
&lt;p&gt;From a theoretical point of view, all this fits perfectly into the system of general &lt;a href="https://en.wikipedia.org/wiki/Good_regulator"&gt;theorems&lt;/a&gt; formulated for all systems, including brains, within the framework of cybernetics. &lt;a href="http://www.hutter1.net/ai/aixigentle.htm"&gt;One of these theorems&lt;/a&gt; states that if we consider the subject and the world with which they interact as computers exchanging information, then making optimal decisions for the subject, which is equivalent to predicting the consequences of available actions, is reducible to the maximum compression of information about the world. Of course, it would be premature to transfer this theorem to the real world since it is based on a rather shaky assumption of universal computability. But it would be equally unwise to ignore its conclusion.&lt;/p&gt;
&lt;p&gt;And now, it's finally time to put everything together. Efficiency necessitates prediction, prediction necessitates generalization, generalization is equivalent to compression... And what is compression? It is the &lt;em&gt;efficiency&lt;/em&gt;&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt; of storing information! The loop is closed, which means that all these processes are equivalent to each other at a fundamental level!&lt;/p&gt;
&lt;h2&gt;Philosophical perspective&lt;/h2&gt;
&lt;p&gt;The conclusion derived above is, of course, not that new. Similar ideas have been put forward in many forms, starting perhaps from an Austrian philosopher &lt;a href="https://www.leopoldina.org/fileadmin/redaktion/Mitglieder/CV_Ernst_Mach_D.pdf"&gt;Ernst Mach&lt;/a&gt; and &lt;a href="https://plato.stanford.edu/entries/ernst-mach/#Sci"&gt;his scientific framework of &lt;em&gt;Denkökonomie&lt;/em&gt;&lt;/a&gt;, which literally translates to “economy of thought”, “economy” meaning savings or efficiency. At its core, this framework demands the simplest possible (most economical) explanation of observed facts in science; it's essentially a beefed-up version of Occam's Razor, which Mach also derived from biological necessity.&lt;/p&gt;
&lt;p&gt;But what do we mean by this complexity/economy quantitatively? One answer is explanatory power, which is expressed as a ratio of the number of observations a theory explains divided by the number of assumptions that it relies on and which are unprovable within it. This is usually enough to compare two theories within the same subject domain. In the context of statistical models, the number of assumptions can be replaced with the number of trainable parameters. But it is possible to generalize even further.&lt;/p&gt;
&lt;p&gt;Recall the concept of &lt;a href="https://brilliant.org/wiki/kolmogorov-complexity/"&gt;Kolmogorov complexity&lt;/a&gt; mentioned earlier. Based on it, a theoretical measure of complexity for scientific theories was suggested by Ray Solomonoff as part of his &lt;a href="https://www.lesswrong.com/posts/Kyc5dFDzBg4WccrbK/an-intuitive-explanation-of-solomonoff-induction#Solomonoff_s_Lightsaber"&gt;theory of inductive inference&lt;/a&gt;. This is, in essence, a mathematical formalism for Mach's philosophical framework. It, in turn, is the basis for &lt;a href="https://www.lesswrong.com/posts/DFdSD3iKYwFS29iQs/intuitive-explanation-of-aixi"&gt;AIXI&lt;/a&gt;: an algorithm that is supposedly capable of discovering the most economic theories automatically; in other words, an AGI. The only issue is that none of these things are computable, which is why they were never used in practice. Some, however, try to &lt;a href="https://arxiv.org/abs/1007.2049"&gt;approximate AIXI&lt;/a&gt;, and this avenue of research probably deserves more attention than it is currently getting.&lt;/p&gt;
&lt;p&gt;It is easy to see that scientific progress almost always goes in the direction of more economical theories, at least when considered &lt;em&gt;on a sufficiently long time scale&lt;/em&gt;. For example, when radioactivity was first described, it did not fit into existing physical theories at all. Initially, it had to be considered as a completely independent phenomenon, which, of course, is not economical in the short term. But new theories soon emerged, combining old physics with new phenomena into a single and more economical system, and at the same time explaining things that previously seemed inexplicable: why are stars burning, and where atoms come from. Mach himself likely developed his framework by trying to formalize this very observation.&lt;/p&gt;
&lt;p&gt;However, most people who know about Denkökonomie today know it not from the source material but from the &lt;a href="https://en.wikipedia.org/wiki/Materialism_and_Empirio-criticism"&gt;critique of it penned by Lenin&lt;/a&gt;. Therefore, it would be unfair to proceed without addressing it first.&lt;/p&gt;
&lt;h3&gt;Lenin’s perspective&lt;/h3&gt;
&lt;p&gt;If we accept that (1) Denkökonomie is a &lt;em&gt;sufficient&lt;/em&gt; criterion of truth and (2) the brain always strives for the most economical explanation of observed facts, then it follows that all people, as they gain experience, should converge on identical ideas about the world. But empirically, this obviously does not happen. This contradiction can be explained in two ways. The way that Mach chose was to deny the materiality of the world. Indeed, if each observer analyzes their own world, then there is no basis for consistency between their models of those worlds. Thus, Mach eventually reasoned himself into solipsism by an overly extreme application of his own principle.&lt;/p&gt;
&lt;p&gt;But ironically, a much more &lt;em&gt;economical&lt;/em&gt; way to explain the same contradiction is to abandon one of Mach's postulates. Since postulate 2 is now backed by science (as explained above), the problem is obviously in postulate 1. That is, Denkökonomie is not a sufficient but a necessary criterion of truth. Put simply, this means the truth cannot be found by following simplicity alone (string theory and supersymmetry empirically demonstrated this), but somehow, the truth always turns out to be simple.&lt;/p&gt;
&lt;p&gt;Having asserted that, we are left with the problem of coordination between the brain and the world outside it. To fill that gap, Lenin developed the dialectical materialist reflection theory, which became the model of intelligence in Marxism. The theory of reflection by itself is much older and simpler than Marxism and, at the base level, alleges that while we can only access objective reality through sensations, the results of these sensations are normally consistent with reality itself. Were they not, there would be no adaptive value in having the sensations and they would have never evolved in the first place. In other words, we have an internal model of reality in our minds that is being constantly clarified and grounded by sensations. The process of perception is then a transformation of a thing-in-itself into a thing-for-us through reflection.&lt;/p&gt;
&lt;p&gt;The problem is that any kind of reflection theory has a hard time explaining specifically how this process of transformation works since the thing-in-itself is obviously far more complex and detailed than the thing-for-us. The vast majority of information is necessarily lost in this process of transformation, so the question is: what is retained?&lt;/p&gt;
&lt;p&gt;For Lenin, the answer is that the information we retain is what is &lt;em&gt;useful in practice&lt;/em&gt;. Although usefulness is rather subjective as it heavily depends on the goal of the actor, many actors pursuing different goals eventually arrive at similar instrumental representations of the world. Thus, the process of reflection converges and becomes deterministic in the limit.&lt;/p&gt;
&lt;p&gt;Take a map as an example. If you download a map of the greater Berlin from Google Maps, it takes up a mere 100 megabytes: less than 30 bytes per capita (what a unit, huh). In that space, Google fits pretty much all the information you will realistically need to live in the city, and yet no one would argue that it encompasses the full reality of Berlin. This trick works because society has developed many useful abstractions, such as roads and buildings, which can be represented with only a few numbers each.&lt;/p&gt;
&lt;p&gt;And what do I need a map of Berlin for? To &lt;em&gt;predict&lt;/em&gt; where a specific train will take me or where I will end up after turning left or right at a specific intersection. So, usefulness is essentially a proxy for predictive power, and it is also connected with compression.&lt;/p&gt;
&lt;p&gt;But this connection goes both ways. We capture the compressible aspects of reality, but then we also use the understanding of these aspects to shape reality itself to be more compressible, like building straight roads and rectangular houses. This idea, if extrapolated, has some surprising and potentially useful implications.&lt;/p&gt;
&lt;h3&gt;A surprise detour into the Fermi paradox&lt;/h3&gt;
&lt;p&gt;Imagine we are writing software for an &lt;a href="https://www.astronomy.com/science/breakthrough-starshot-a-voyage-to-the-stars-within-our-lifetimes/"&gt;interstellar microprobe&lt;/a&gt; that will be launched to perform close-up imaging of a potentially inhabited exoplanet. Problem is, we cannot transmit the image back home for analysis: the probe has neither the antenna size nor the power budget for that. All we can send back is a single number: the probability that the planet hosts a technological civilization. How do we compute it?&lt;/p&gt;
&lt;p&gt;Leaning on what was said in the previous section, we can assume that aliens, like us, will build regular (and therefore compressible) structures, because they, no matter who "they" are, will find regularity more &lt;em&gt;useful&lt;/em&gt; than chaos. And, luckily for our probe's processor, this regularity can be easily measured without any assumptions about the form and function of structures on an image by simply calculating its &lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy"&gt;entropy&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Long-time readers know that I've long been obsessed with the Fermi paradox, and specifically with deriving an answer to it from the definition of intelligence. In &lt;a href="https://cyberape.space/content/pages/black-attractor/paper.pdf"&gt;my paper on the topic&lt;/a&gt; I started with three different definitions and then expressed them through each other (so far only in the limit). And it is no coincidence that entropy was also the thread connecting these definitions.&lt;/p&gt;
&lt;p&gt;But let us also consider counterarguments to my proposition, which can me made from two sides.&lt;/p&gt;
&lt;p&gt;On the one hand, not everything that is compressible is artificial. Simple periodic patterns often occur in nature without life being involved, such as snowflakes or &lt;a href="https://askanearthspacescientist.asu.edu/top-question/columnar-jointing"&gt;basalt columns&lt;/a&gt;. Crystals in general are great at making these and, given the right conditions, could form structures visible from space. Then there are fractals: a class of patterns that are extremely compressible and are widely used in both nature and technology.&lt;/p&gt;
&lt;p&gt;On the other hand, the highly regular structures that we are building today require lots of effort to maintain. It is possible to imagine that, as technology evolves in the direction of self-replication and self-maintenance, technological structures become chaotic again, converging with nature. This idea has lots of potential for further development, and I hope to come back one day to give it a good think.&lt;/p&gt;
&lt;h3&gt;Platonic perspective&lt;/h3&gt;
&lt;p&gt;Slightly earlier I made an unsubstantiated claim that the category of &lt;em&gt;"useful"&lt;/em&gt; information is, in the limit, independent of the goal for which it is used. This, if true, is a deep and fundamental conclusion about the structure of reality itself, so it requires some justification.&lt;/p&gt;
&lt;p&gt;It is a well-known empirical fact in the deep learning community that training a model on the task you actually want to perform is not always a great start. Often it benefits to first train it on a more general task (known as pretraining) before specializing it to your use case (known as fine-tuning). For example, a language model that is first trained to understand multiple languages and then fine-tuned to translate between them performs much better compared to the same model trained on examples of translation from the start. And you can understand why, if you scroll back to the example of word embeddings for two languages overlayed on each other. This implies that the same representations of data are useful for many different tasks; in other words, they &lt;em&gt;converge&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This convergence for language and vision models has been recently measured in an already famous paper &lt;a href="https://arxiv.org/abs/2405.07987"&gt;"The Platonic Representation Hypothesis"&lt;/a&gt;. ChatGPT-4o heavily leverages it by using the same internal representations for different data types. There are also examples of representations convergence that are much harder to comprehend, such as a &lt;a href="https://www.simonsfoundation.org/event/the-next-great-scientific-theory-is-hiding-inside-a-neural-network/"&gt;model for solving partial differential equations that performs better after pretraining on... cat videos?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is hopefully enough to show that the convergence of representations between different tasks is empirically real. On the theoretical side, the explanation of this effect is offered again by the Manifold hypothesis. If real (or &lt;em&gt;useful&lt;/em&gt;) data always lies on low-dimensional latent manifolds, then jumping from one manifold to another (fine-tuning) will always be easier than finding the manifold in random noise, which is the alternative to pretraining.&lt;/p&gt;
&lt;p&gt;On the philosophical side, this looks a lot like Platonic idealism, which posits the existence of Forms: ideal representations that precede and give rise to material objects. Similar ideas can be found in Eastern philosophies. While this does not imply that we need to reject materialism in favor of objective idealism, it definitely posits some questions that will be very hard to answer from the materialistic perspective. But I am sure that it will be worth it.&lt;/p&gt;
&lt;p&gt;And if we once again allow ourselves to make analogies between neural networks and human brains, then this conclusion gives us an interesting perspective on the education system. I often hear people complaining about learning trigonometry in school, which no one ever uses after graduation. And fair enough, even I, a professional mathematician, rarely use it (and certainly look up trig identities instead of trying to dig them from the depths of memory). What is the point of learning such seemingly useless things? Could this be a form of &lt;em&gt;pretraining&lt;/em&gt; that helps in life later in ways we do not consciously recognize? I suspect so, because people who did not graduate school empirically end up performing worse in a wide range of tasks, meaning that the school definitely does something useful. We just don't have the language to explain what remains in the head of an educated person after all the trig identities, poems and historical dates are no longer there.&lt;/p&gt;
&lt;h3&gt;Copernican perspective&lt;/h3&gt;
&lt;p&gt;I must add, however, that there are situations where Denkökonomie &lt;em&gt;can&lt;/em&gt; be used as a criterion of truth, namely, all else being equal. For example, if we put ourselves in the shoes of people who do not yet know about Newton's theory of universal gravitation, then how should we choose between the geocentric and heliocentric cosmological models? Both give fairly accurate predictions, so it is impossible to reject one of them by experiment alone. What is the difference between them? The heliocentric model uses a fixed and small number of parameters to describe orbits, while the geocentric model, in order to achieve the same accuracy, requires winding epicycles onto epicycles, thereby generating many more parameters. Later, Joseph Fourier invented one of the most powerful tools of mathematics: the Fourier transform, which allows one to describe &lt;em&gt;any&lt;/em&gt; trajectory or signal in a similar way. Hence, the geocentric model is bad not because it is incorrect but because it does not actually contain information about the motion of the planets: all this information is stored in its parameters. The heliocentric model &lt;em&gt;compresses&lt;/em&gt; this information, &lt;em&gt;saving&lt;/em&gt; parameters, and this is what makes it preferable. In other words, the heliocentric model has greater explanatory power.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/embeddings/centrism.png"&gt;&lt;/p&gt;
&lt;p&gt;While we should strive to use “Copernican modeling” wherever possible, there are problems where this approach fails, and we are forced to resort to “Ptolemaic modeling.” Statistics and machine learning are essentially sciences that study how to do it properly.&lt;/p&gt;
&lt;p&gt;The price for using over-parametrized modeling is usually the model's inability to &lt;em&gt;generalize&lt;/em&gt;. In order to at least somewhat regain it, regularization and dimensionality reduction methods were invented. They can be described as artificially limiting the complexity of the model in terms of the number of parameters or their values, in essence, forcing the model to be more &lt;em&gt;economical&lt;/em&gt;. These methods come dangerously close to using Denkökonomie as a &lt;em&gt;sufficient&lt;/em&gt; criterion of truth, but practice shows that they work, and it is practice that is the ultimate criterion of truth.&lt;/p&gt;
&lt;h3&gt;Hegelian perspective&lt;/h3&gt;
&lt;p&gt;But let us return to the contradiction in Mach's philosophy. Even having rejected Denkökonomie as a sufficient criterion of truth, we are still left with the question: why do different people come to completely different ideas about the world if its economical (compressed) representation, which the brain seeks to find, must be objective and independent of the observer? To answer this, we will have to turn to another branch of philosophy: dialectics.&lt;/p&gt;
&lt;p&gt;What we need from dialectics specifically is the concept of “&lt;a href="https://en.wikipedia.org/wiki/Aufheben"&gt;&lt;em&gt;Aufhebung&lt;/em&gt;&lt;/a&gt;,” which I will translate as “overcoming,” although no perfect translation to English exists. A classic example that demonstrates this concept is found in the relationship of the general theory of relativity and quantum mechanics with classical mechanics. These more advanced theories extend beyond classical mechanics, yet they also preserve it as a special case in the limit: when the speed of light approaches infinity and the Planck constant approaches zero, respectively. Technically, we can say that these theories are &lt;em&gt;contradictory&lt;/em&gt; because, &lt;em&gt;empirically,&lt;/em&gt; the speed of light is not infinite, and Planck's constant is not zero. However, the important thing is that more advanced theories can usually simulate less advanced ones through similar thought experiments.&lt;/p&gt;
&lt;p&gt;The procedure of dialectical overcoming is invaluable in theoretical disputes, where the opponent will always present facts that do not fit into your theory. If you cannot use it, you will eventually find yourself sandwiched between two losing options: challenging the facts or constraining the scope of your theory. But the ability to overcome such contradictions not only allows you not to lose an argument but also turns the dispute into a full-fledged research activity that can potentially lead to new discoveries. In such a dispute, truth can really be born.&lt;/p&gt;
&lt;p&gt;However, the easiest way to trace the process of dialectical overcoming is not in discussions between different people but in the process of development of a single person: oneself. Each of us, in the depths of our closets or hard drives, has some notes from ancient times, looking at which we can &lt;s&gt;die of cringe&lt;/s&gt; understand how our worldview developed. And the basic law of this development is the preservation of previous versions of ourselves that we overcame. We never throw away all previous experience to start from scratch. Even when radically changing our views, in the new ones, we retain an imprint of the old ones, which strengthens them. For example, it allows us to argue for the new worldview much more competently compared to those for whom it is the starting point. The same logic explains &lt;a href="https://bigthink.com/mind-brain/antifragility/"&gt;the &lt;em&gt;antifragility&lt;/em&gt; of the human mind&lt;/a&gt;: it is by &lt;em&gt;overcoming&lt;/em&gt; (in a dialectical sense) pain and hardship that we become better as people. And I also believe that somewhere here lies the answer to the &lt;a href="https://www.cs.uic.edu/~liub/lifelong-learning/continual-learning.pdf"&gt;problem of continual learning&lt;/a&gt;, but extracting it will take some more work.&lt;/p&gt;
&lt;p&gt;For the same reason, not a single generally accepted theory in modern positive science can be &lt;em&gt;refuted&lt;/em&gt;. The only way to advance further in knowledge is to &lt;em&gt;overcome&lt;/em&gt; the old theory, that is, to find a more general theory, a special case of which will be the old one. But since generalization, as we found out above, is equivalent to compression, it turns out that &lt;em&gt;the result of dialectical overcoming should be more economical&lt;/em&gt;. And since dialectical overcoming is also a process of removing contradictions, the converse proposition can be formulated: &lt;em&gt;the more economical our theories are, the fewer contradictions there are between them&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The same logic explains why persuading people generally doesn't work. It is impossible to produce an “embedding” that can natively integrate into another person's “latent space” without access to their subjective experience, which is inaccessible to us by definition. Only I myself can convince myself of something, either in order to resolve the contradiction in my current model of reality or &lt;a href="https://therussiaprogram.org/ps_lab_1"&gt;under the influence of material factors&lt;/a&gt;. Socrates’ “maeutics” is based on the first option: it consists of identifying contradictions in the interlocutor’s worldview and making them obvious. Sometimes, this leads to the interlocutor reflecting and changing their opinion in order to resolve the contradiction. However, it is impossible to guarantee persuasion, let alone in a specific direction. Socrates was fine with that: in his gnoseology, each person already contained true knowledge about the world a priori, and all he had to do was to extract it. But that view leads to the same dead end in which Mach later ended up.&lt;/p&gt;
&lt;p&gt;Another significant drawback to this architecture is that it’s &lt;em&gt;monolithic&lt;/em&gt;, meaning that it’s impossible to transfer a part of knowledge from one model to another. Returning to the example of embeddings, we can see a manifestation of this problem in that the embeddings themselves are useless without the model that generated them. For the same reason, it is impossible to simply “download” the knowledge accumulated by humanity into the brain of an individual person. Each brain is a new model, and each must independently build its “latent space” from scratch. For this, it must conceptually go through the path that its predecessors already took (this is indirectly confirmed by the above studies on mind reading, where a model must be fitted to each subject). The development of a person repeats in miniature the development of the entire civilization in which they happened to be born. This, by the way, imposes an unpleasant limitation on science itself: sooner or later, the time required to retrace the path of humanity, even in its most compressed form, will exceed life expectancy, and new generations simply will not have enough time to create something original; therefore, artificial life extension or other ways of overcoming this limitation (e.g., hiveminds) will become a necessary condition of further progress. But today may be too early to worry about it.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Actually, colors form not a linear space but a symmetry group SU(3). However, in the interval [0;1], it behaves in the same way as a linear space, and it is in this interval that we usually work with colors. Just be careful with subtraction. Also, I am treating colors as a perceptual category rather than a physical one, so spare the criticisms from the perspective of physics. I have a &lt;a href="color.html"&gt;post&lt;/a&gt; about that distinction, but it is not translated to English.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Here, I deliberately omit the rather important difference between lossless and lossy compression. Most embedding models are not designed to restore the original object using the embedding itself, i.e., if they implement compression, it is very lossy. But this clarification becomes unnecessary when we move on to the discussion of human memory, in which these two types of information storage differ more quantitatively than qualitatively.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;You can also recall the story of convolutional neural networks (CNN), which replicate the structure of the mammalian visual cortex. But in this case, people &lt;a href="https://doi.org/10.1162/jocn_a_01544"&gt;deliberately “copied” an architecture from nature&lt;/a&gt;, which is much less interesting than the independent (convergent) emergence of the same architecture in nature and technology.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;Here I can be accused of substituting the thesis: at the beginning, it was about minimizing thinking, and at the end about minimizing memory. But these things are quite related. Once an economical (compressed) model has been created, cognitive operations with it also simplify, becoming more economical.&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Пост"></category></entry><entry><title>Nothing personal, just business</title><link href="https://cyberape.space/en/war.html" rel="alternate"></link><published>2022-03-13T00:00:00+00:00</published><updated>2022-03-13T00:00:00+00:00</updated><author><name>FunBotan</name></author><id>tag:cyberape.space,2022-03-13:/en/war.html</id><summary type="html">&lt;p&gt;Looking for the reasons behind the unreasonable&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;People always have been the foolish victims of deception and self-deception in politics, and they always will be, until they have learned to seek out the interests of some class or other behind all moral, religious, political and social phrases, declarations and promises.&lt;/p&gt;
&lt;p&gt;— Vladimir Lenin&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Easy as it is to apply this great principle in hindsight, to events long past and well studied. It's another matter entirely to do so in a state of shock and awe that we have all found ourselves in the morning of February 24, 2022.&lt;/p&gt;
&lt;p&gt;At first glance, it may seem that the invasion of Ukraine by Russia cannot possibly have a rational, materialistic justification. Too high are the costs Russia will have to pay even in the case of total victory (which itself is becoming less and less plausible by the day), let alone in the case of defeat. What could possibly motivate the Russian ruling class to go for broke to this extent?&lt;/p&gt;
&lt;p&gt;It's no secret that the &lt;a href="https://atlas.cid.harvard.edu/explore?country=186&amp;amp;product=undefined&amp;amp;year=2019&amp;amp;productClass=HS&amp;amp;target=Product&amp;amp;partner=undefined&amp;amp;startYear=undefined"&gt;primary exports of Russia are oil and gas&lt;/a&gt;. Revenue from their sale comprises half of the state budget and 30% of GDP. The Russian regime has put all of its bets on the carbon horse, &lt;a href="https://www.rt.com/business/541415-russia-oil-reserves-decline/"&gt;and now that horse is getting tired&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To understand how Ukraine comes into the picture, we need to go back to 2012, when &lt;a href="https://www.iene.gr/6thSEEED/articlefiles/sessionIII/Hutta.pdf"&gt;huge offshore gas deposits were discovered in its territorial waters around Crimea&lt;/a&gt;. Around the same time, fracking was developed in the USA, which opened access to &lt;a href="http://shalegas.in.ua/en/shale-gas-resources-in-ukraine/"&gt;shale gas deposits under Donbas and Transnistria&lt;/a&gt;. Can it be just a coincidence that the same regions have the highest levels of ethnic tensions and separatist sentiments?&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/war/basis.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Ukraine, however, lacked the domestic capital and technology to extract all these resources. This naturally pushed the Ukrainian capital into the embrace of the American one: &lt;a href="https://www.offshore-technology.com/uncategorised/newsexxon-consortium-ukraine-skifska-oil-gas-field/"&gt;Yanukoviche's government began issuing drilling permits to such companies as Shell and Exxon&lt;/a&gt;. Had the Russian capital left the situation to be handled by the "invisible hand of the market," Ukraine might have become the second-largest exporter of gas in Europe in a few years, or maybe even push Russia out of the market: Western Europe would have preferred to buy gas from convenient Ukraine rather than wild and unpredictable Russia. And from there, it wouldn't be a stretch to imagine Ukraine in the EU or even NATO.&lt;/p&gt;
&lt;p&gt;Had Yanukovich and his clique retained their power, they would broker a deal with Russian capital. But then 2014 happened: American capital had openly entered the game, raising the stakes tremendously. Russia replied by &lt;a href="https://euromaidanpress.com/2018/10/10/black-sea-gas-deposits-an-overlooked-reason-for-russias-occupation-of-crimea/"&gt;annexing Crimea&lt;/a&gt; and &lt;a href="https://www.bbc.com/ukrainian/ukraine_in_russian/2015/12/151216_ru_s_ukraine_russia_sea"&gt;immediately proceeding with offshore drilling in its waters&lt;/a&gt;. As for the shale deposits, &lt;a href="https://www.euractiv.com/section/energy/opinion/russia-s-silent-shale-gas-victory-in-ukraine/"&gt;their development was effectively stalled by civil war&lt;/a&gt;, which Russia was satisfied with at the time: it didn't have the fracking technology to begin with.&lt;/p&gt;
&lt;p&gt;Just to relieve any doubts, in 2014, &lt;a href="https://tass.ru/mezhdunarodnaya-panorama/6964280"&gt;Joe Biden had placed his son on the board of directors of the largest Ukrainian oil and gas holding&lt;/a&gt;. Also, &lt;a href="https://vesma.today/news/post/36142-ukrainskiy-milliarder"&gt;one Ukrainian oil oligarch has committed suicide shortly after the war started&lt;/a&gt;, which might not prove anything by itself, but fits quite nicely into the bigger picture.&lt;/p&gt;
&lt;p&gt;Why were the Russian elites waiting for eight years to continue the war? That's not entirely clear at the moment. One plausible reason is the unprecedented growth of gas prices in Europe, which made imposing sanctions harder for the European economy than ever. Another possibility is the dire need for fresh water in Crimea: not coincidentally, &lt;a href="https://youtu.be/Gi6EYIS7isk"&gt;one of the first targets of the Russian military was the dam blocking the North-Crimean canal&lt;/a&gt;. The peninsula wouldn't survive another 2020-level drought. Ironically, the reason behind the severity of that drought was climate change, which is a direct effect of the very industry that fuels Russian aggression.&lt;/p&gt;
&lt;p&gt;Beside that, in 2005, 80% of Russian gas exports to Europe were pumped through pipelines lying in Ukraine, which were only supposed to be decommissioned by 2024. Despite the war, &lt;a href="https://ria.ru/20220301/gaz-1775733192.html"&gt;these pipelines are still working&lt;/a&gt;, and &lt;a href="https://www.gazprom.ru/investors/disclosure/actual-supplies/"&gt;their utilization is only growing&lt;/a&gt; along with gas prices. Apparently, the Russian capital has deemed Nord Stream 2 an acceptable sacrifice for regaining control over the older pipes that Ukrainians will no longer demand a transit fee to use. And even if a stray shell bursts a pipe here or there, it will only further raise gas prices, making the whole affair even more profitable.&lt;/p&gt;
&lt;p&gt;One might object: "why, then, are &lt;a href="https://www.kommersant.ru/doc/5240226"&gt;those oil and gas oligarchs publicly denounce the war&lt;/a&gt;?" Well, that is one of the more straightforward questions: they are simply preparing an exit strategy for themselves. This is precisely what happened at the Nuremberg tribunal: &lt;a href="https://youtu.be/oyJTv_qLqsI"&gt;German industrialists managed to exit unscathed by pretending to be hostages of the Nazi regime while actually being its beneficiaries&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, fuel is not the only reason for this conflict. The annexation of Crimea can also be explained by the strategic importance of Sevastopol (being the only warm-water port of the Russian navy in Europe); and the current war was justified by many with fears of NATO expansion. But let's ask ourselves the question: why does the Russian military even need these strategic footholds? To defend &lt;em&gt;what,&lt;/em&gt; exactly? Certainly not the Russian people, who have been ravaged by COVID-19 for two years now with very little concern from the government. No, the reason a capitalist country needs a military in the first place is to defend the interests of its capital. And which capital has interests important enough to justify an all-out war? &lt;em&gt;Only the carbon capital.&lt;/em&gt; &lt;a href="http://energy-cg.com/UkraineAtRisk.html"&gt;This is why the fight for fossil fuels is the &lt;em&gt;axial&lt;/em&gt; reason behind the war&lt;/a&gt;, meaning that all other causes are merely strung on it.&lt;/p&gt;</content><category term="Пост"></category></entry><entry><title>Let me to the moon, Mom!</title><link href="https://cyberape.space/en/apollo11.html" rel="alternate"></link><published>2019-07-21T00:00:00+01:00</published><updated>2019-07-21T00:00:00+01:00</updated><author><name>FunBotan</name></author><id>tag:cyberape.space,2019-07-21:/en/apollo11.html</id><summary type="html">&lt;p&gt;The 50th anniversary of the moon landing is an excellent opportunity to take a fresh look at our past and future in space.&lt;/p&gt;</summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;That's one small step for [a] man, one giant leap for mankind.&lt;br&gt;
— Neil Armstrong, 21.07.1969&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This phrase was uttered precisely 50 years ago. Many consider it the most important phrase in history (or maybe the next most important one after «Поехали!»). Many people recognize it even without speaking English. Entire scientific articles have been written about whether it actually contained the indefinite article [a]. And there are serious reasons for all this.&lt;/p&gt;
&lt;p&gt;Despite its militant beginnings in the form of nuclear ICBMs, space exploration always seemed to be the purest, most selfless, and most inspiring vector of technological development. There is no Chornobyl or Facebook in space for critics to point their fingers at as examples of how things can go wrong. People can die in space, sure, but only the people who voluntarily signed up for it, not random, unsuspecting civilians who won’t know what happened until it’s too late. It is difficult to even imagine how that could happen. I can’t remember a single work of science fiction where space exploration &lt;em&gt;itself&lt;/em&gt; would lead humanity to disasters without involving aliens, anomalies, and other external forces. Is that even possible?&lt;/p&gt;
&lt;p&gt;Surely, someone asked similar questions regarding the Internet in the 90s. Can access to all the knowledge in the world &lt;em&gt;narrow&lt;/em&gt; a person’s horizons? Can the ability to communicate with anyone, anywhere, give rise to intolerance? Now we know the paradoxical answer. Social networks alone have probably led to more deaths and disabilities than Chornobyl through acquired mental illnesses and suicides.&lt;/p&gt;
&lt;p&gt;Optimists tend to only imagine the desirable results of applying new technologies rather than the realistic ones, to ignore the &lt;em&gt;achievability&lt;/em&gt; of the desired result in favor of its &lt;em&gt;possibility&lt;/em&gt;. You can draw an infinite number of physically possible future scenarios, but only those from which you can draw a continuous path to the present can be realized. Moreover, as a general rule, the path of least resistance always leads to dystopia.&lt;/p&gt;
&lt;p&gt;For example, it was naive to think that if you give people the Internet, they will start using it for self-education. Some may do, but most will follow the path of least resistance and simply solve their everyday tasks using the Internet: watch the news, order food, and search for partners. Demand creates supply, the industry is left to the whims of the market, and the market will find a way to create a dystopia.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/apollo11/stonks.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The first decades of space exploration were so positive precisely because they &lt;em&gt;didn’t&lt;/em&gt; take the path of least resistance. When Korolev persuaded Khrushchev to start a space program; when Kennedy said, “We &lt;em&gt;choose&lt;/em&gt; to go to the Moon,” they actively steered history, taking it away from the path of least resistance with the sheer power of free will. But they could only maintain the new course for so long.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/apollo11/kennedy.jpg"&gt;&lt;/p&gt;
&lt;p&gt;One could, of course, argue that Elon Musk also made a free choice when creating SpaceX and that he is now at the helm. But, firstly, I doubt that his choice was genuinely free. Secondly, behind Khrushchev and Kennedy stood the People. Behind private space corporations stands only the money. This means that they will inevitably follow the path that allows them to earn as much money as possible: the path of least resistance.&lt;/p&gt;
&lt;p&gt;It’s just that until recently, no one imagined this path leading into space. And the dreamers, who have been waiting for the continuation of the space epic for 50 years, are too intoxicated by success to notice the catch. They expect a logical continuation of the space race of the 20th century, and so far, corporations &lt;a href="https://dearmoon.earth/"&gt;have successfully masqueraded&lt;/a&gt; under this expectation. Awareness of reality will come too late, having gone through all the stages of accepting the inevitable. The ship of history will already be caught up in a powerful current, and it will become impossible to steer out of it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A scientific colleague tells me about a recent trip to the New Guinea highlands where she visited a stone age culture hardly contacted by Western civilization. They were ignorant of wristwatches, soft drinks, and frozen food. But they knew about Apollo 11. They knew that humans had walked on the Moon. They knew the names of Armstrong and Aldrin and Collins. They wanted to know who was visiting the Moon these days.
— Carl Sagan&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Life works so that its worst moments are always ahead, regardless of our actions; however, nothing prevents the best moments from also being there: you just need to make an effort. I argue that the same can be said about the life of civilization as a whole.&lt;/p&gt;
&lt;p&gt;Strongly paraphrasing Tsiolkovsky, we can say that the planet is a parental home of civilization, in which much is given for free but in which we will never be truly free. You can maintain good relationships with your family for a long time, achieve financial independence, and even support your parents with your own income, but sooner or later, you have to leave the home.&lt;/p&gt;
&lt;p&gt;As a civilization, we have already completely ruined our relationship with our parents, and now it is only a matter of time before they force us out of the door. We can move (colonization of space), or we can correct our mistakes and try to restore the relationship (for example, by creating an artificial climate control system in space), but in both cases, we need the skills to live independently (autonomous spaceships).&lt;/p&gt;
&lt;p&gt;Yes, life in space will not be anything like the rosy fantasies of Star Trek. Many dark pages of history are yet to be written. Many nightmares beyond comprehension are closely watching us from the interstellar void, waiting for a chance to take a bite. And realizing this is an essential step on the path to real adulthood. But this is not a reason to hate space or be offended by Elon Musk. Like &lt;a href="https://www.atomicarchive.com/resources/documents/beginnings/einstein.html"&gt;Einstein in 1939&lt;/a&gt;, he does what must be done, even if he will regret it in retrospect.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/apollo11/darkside.png"&gt;&lt;/p&gt;
&lt;p&gt;But what role does Apollo 11 play in this whole analogy? The first cautious step into the adult world, accompanied by obviously unrealistic expectations and indescribable pleasure? The answer suggests itself. This is the first teenage love. A successful and happy, albeit unsustainable and short-lived one.&lt;/p&gt;
&lt;p&gt;Continuing the analogy, we may conclude two things. The first is that we will never have anything like Apollo 11 again. And the second is that something much better may be waiting up ahead.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/apollo11/love.png"&gt;&lt;/p&gt;</content><category term="Праздничный пост"></category></entry><entry><title>The «Black Attractor» Fermi paradox solution</title><link href="https://cyberape.space/en/fermi-paradox.html" rel="alternate"></link><published>2018-04-22T00:00:00+01:00</published><updated>2018-07-02T00:00:00+01:00</updated><author><name>FunBotan</name></author><id>tag:cyberape.space,2018-04-22:/en/fermi-paradox.html</id><summary type="html">&lt;p&gt;A summary of what I believe is the ultimate set of solutions to the paradox.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;This post is deprecated and left here for the sake of history. If you are interested in the topic, I strongly recommend reading &lt;a href="https://cyberape.space/content/pages/black-attractor/paper.pdf"&gt;this much more refined article&lt;/a&gt; instead.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;strong&gt;Fermi paradox&lt;/strong&gt;, named after physicist Enrico Fermi, is the apparent contradiction between the lack of evidence and high probability estimates for the existence of extraterrestrial civilizations. The basic points of the argument, made by physicists Enrico Fermi and Michael H. Hart, are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;There are billions of stars in the galaxy that are similar to the Sun, and many of these stars are billions of years older than the Solar system.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;With high probability, some of these stars have Earth-like planets, and if the Earth is typical, some may have developed intelligent life.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Some of these civilizations may have developed interstellar travel, a step the Earth is investigating now.
Even at the slow pace of currently envisioned interstellar travel, the Milky Way galaxy could be completely traversed in a few million years.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;According to this line of reasoning, the Earth should have already been visited by extraterrestrial aliens. In an informal conversation, Fermi noted no convincing evidence of this, leading him to ask, "Where is everybody?" (&lt;a href="https://en.wikipedia.org/wiki/Fermi_paradox" target="_blank"&gt;Wikipedia&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Having studied the vast majority of proposed Fermi paradox solutions, I believe I had found the common misconception that plagues not just them, but multiple seemingly unrelated theories as well. The assumption of rationality.&lt;/p&gt;
&lt;p&gt;But first things first. Why &lt;em&gt;should&lt;/em&gt; we see any signs of alien activity? Because, whatever their logic and motives may be, they should definitely share a common trait: the tendency for growth. It is either included or follows directly from any definition of life. This implies that any life either stops expanding outward at a certain stage, goes extinct, or simply never occurred before. The latter proposition seems extremely improbable given the age of the universe, and the two former ones are more or less the same, since stagnation eventually means running out of energy and dying. The more you think about it, the scarier this idea becomes.&lt;/p&gt;
&lt;p&gt;For now let us assume that our competitors are either dead or never born; that we are the only life within our cosmological horizon; that we are free to make our own choices. &lt;em&gt;Or are we?&lt;/em&gt; Can a civilization really make a deliberate choice, especially one that makes logical sense? If we could, wouldn't we already stop wars, hunger, diseases and climate change? We certainly have the science and resources to do so. So why wouldn't we?&lt;/p&gt;
&lt;p&gt;Because, like any system, society is &lt;em&gt;not&lt;/em&gt; a sum of its members. It is an entity of its own, functioning in accordance with its own rules which couldn't care less about individual humans. Even an ideal totalitarian dictator cannot control the society as a system, let alone the will of the majority. Even though humans are orders of magnitude smarted than ants, &lt;em&gt;humanity&lt;/em&gt; isn't much smarter than an anthill when it comes to making decisions. This is what &lt;a href="https://youtu.be/oo4YAYg68OU" target="_blank"&gt;Noam Chomsky calls "Institutional irrationality"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It actually presents an advantage for science fiction writers: even if it's impossible to imagine a being far smarter than oneself, predicting how those beings will behave as a civilization is very much on the table.&lt;/p&gt;
&lt;p&gt;So, how will &lt;em&gt;humans&lt;/em&gt; behave in the future? To preserve your likely fading attention, I now have to include a chart that explains my vision for the Fermi paradox, that I will then explain step-by-step:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/fermi-paradox/fermi_en.png"&gt;&lt;/p&gt;
&lt;p&gt;We've already concluded that, even if humans aren't the first, other species don't seem to have survived long enough to greet us; so our destiny is up to ourselves. The question then is, "Can we behave rationally?", or "Is there a 'cure' for institutional irrationality?". Both answers lead to fascinating conclusions.&lt;/p&gt;
&lt;p&gt;First, what if we can't? In short term this would mean suffering all the consequences of climate change and possibly a nuclear war, but is that really enough to completely eradicate humanity? I'm not sure about that. Even a small surviving population can quickly rebuild due to information being virtually indestructible at this point. And even if all humans are dead, other species will be happy to take our place. With more than a billion years of mild sunlight remaining, they should have more than enough time to develop intelligence of their own. Hell, they might even evolve space-faring capability through sheer brute force of natural selection! And being wiped out by superintelligent AI only exacerbates the situation, because that AI would have all our faults embedded it its reward function. It would also be life.&lt;/p&gt;
&lt;p&gt;But don't you worry yet, there's another way of destroying ourselves that doesn't rely on sterilizing one particular planet: the way suggested in &lt;a href="https://myanimelist.net/anime/2001/Tengen_Toppa_Gurren_Lagann" target="_blank"&gt;Tengen Toppa Gurren-Lagann&lt;/a&gt;. This must sound like a joke, but bear with me for a while.&lt;/p&gt;
&lt;p&gt;One aspect of institutional irrationality is income inequality. It is not a problem with human civilization in particular, but rather a representation of a more general rule: when tomorrow's state of a variable is directly proportional to today's, its distribution follows &lt;a href="https://en.wikipedia.org/wiki/Pareto_distribution" target="_blank"&gt;Pareto's curve&lt;/a&gt;. There are good reasons to believe that space economy will follow the same rules: the more fuel you have on a spaceship, the further you can go, the more fuel you can mine on-site, etc. What will wealth be measured in? Matter. With advanced enough technology, the kind of matter doesn't make much difference: nearly everything can be used as fuel or construction material. It then naturally follows that everyone would work toward hoarding as much matter as possible and that some will have exponentially more than others. But what happens when you put too much matter in one place, according to relativity?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A black hole happens.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Black holes are the most likely graves for those who came before us, as well as ourselves. A perfect way to make it seem as if we were alone. This isn't even a "Great filter" anymore, but rather a "Great attractor" — the inevitable trap that every civilization has to fall into, sooner or later.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/fermi-paradox/1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;What would be the point of gathering all the matter in such close proximity to enable a collapse? Simply put, protection. The smaller the surface area of an object in space, the easier it is to defend. "Defend from whom?" Again, extrapolation of our present reality yields this answer easily.&lt;/p&gt;
&lt;p&gt;This hypothesis isn't completely untestable, though. Detecting black holes that don't fit our models of galaxy and star formation would be evidence in favor of it. And those black holes &lt;a href="https://physics.aps.org/featured-article-pdf/10.1103/PhysRevLett.116.061102" target="_blank" title="Wow, a link to a legitimate scientific article!"&gt;may actually exist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Would an entire civilization collapse into that one black hole? Almost certainly not. But those who remain after the catastrophe won't have much resources to rebuild. And when they do rebuild, the same exact catastrophe will repeat. This cycle effectively makes infinite exponential growth — the bedrock of Dyson dilemma, and, consequently, the Fermi paradox — impossible.&lt;/p&gt;
&lt;p&gt;In the process of gathering enough matter we are also likely to devour other inhabited solar systems that haven't yet evolved to our level, the same way a construction crew demolishes anthills before building real estate on their place. The scale of destruction is hard to estimate now, but it might well envelop the entire supercluster. Destroying the entire universe, as suggested by the chart, is theoretically impossible, but it doesn't make much difference. The most obvious counterargument, "But surely an interstellar civilization won't be dumb enough to collapse itself into black holes!", contradicts the assumption we've made by going down this logical branch on the chart above, namely "We can't behave rationally".&lt;/p&gt;
&lt;p&gt;But what if we can?&lt;/p&gt;
&lt;p&gt;Well, even if &lt;em&gt;we&lt;/em&gt; can, &lt;em&gt;others&lt;/em&gt; probably don't. And we cannot sit idly by as some alien species is devouring &lt;em&gt;our&lt;/em&gt; supercluster. The best thing we can do to benefit everyone is to forcefully stagnate their development, trap them in their home solar system before it's too late. At least that seems to be the only alternative to complete eradication of all alien life.&lt;/p&gt;
&lt;p&gt;The Zoo hypothesis and the Matrix now seem much more feasible. Maybe, we aren't alone after all? Maybe, our world isn't really &lt;em&gt;ours&lt;/em&gt;? Actually, that might be the best option of all. In this case, we are alleviated from the privilege of making the hard decisions and hard mistakes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://cyberape.space/en/content/posts/fermi-paradox/2.jpg"&gt;&lt;/p&gt;</content><category term="Post"></category></entry></feed>